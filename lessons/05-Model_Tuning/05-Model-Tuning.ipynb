{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0474d5",
   "metadata": {},
   "source": [
    "# Model Tuning: A Comprehensive Guide\n",
    "\n",
    "---\n",
    "\n",
    "<center><h2>Lesson 05</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2bbea",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "1. **Understand the fundamentals** of model tuning and hyperparameter optimization\n",
    "2. **Implement and tune** Multi-Layer Perceptrons (MLPs) for classification and regression\n",
    "3. **Apply Random Forest** algorithms with proper hyperparameter tuning strategies\n",
    "4. **Utilize Support Vector Machines (SVM)** with different kernels and optimization techniques\n",
    "5. **Deploy XGBoost** models with advanced hyperparameter tuning methods\n",
    "6. **Interpret model predictions** using SHAP (SHapley Additive exPlanations) values\n",
    "7. **Compare and select** appropriate models for different types of problems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb602a",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Model Tuning](#introduction)\n",
    "2. [Dataset Preparation](#dataset)\n",
    "3. [Multi-Layer Perceptrons (MLPs)](#mlp)\n",
    "4. [Random Forest](#random-forest)\n",
    "5. [Support Vector Machines (SVM)](#svm)\n",
    "6. [XGBoost](#xgboost)\n",
    "7. [Model Interpretability with SHAP](#shap)\n",
    "8. [Model Comparison and Selection](#comparison)\n",
    "9. [Conclusion and Best Practices](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3636a0e",
   "metadata": {},
   "source": [
    "## 1. Introduction to Model Tuning {#introduction}\n",
    "\n",
    "### What is Model Tuning?\n",
    "\n",
    "Model tuning, also known as hyperparameter optimization, is the process of finding the optimal configuration of hyperparameters for a machine learning model to achieve the best performance on your specific dataset.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Parameters**: Learned from data during training (e.g., weights in neural networks)\n",
    "- **Hyperparameters**: Set before training and control the learning process (e.g., learning rate, regularization strength)\n",
    "- **Cross-validation**: Technique to assess model performance and avoid overfitting\n",
    "- **Grid Search**: Exhaustive search over specified parameter values\n",
    "- **Random Search**: Random sampling of hyperparameters\n",
    "- **Bayesian Optimization**: Smart search using probabilistic models\n",
    "\n",
    "### Why is Model Tuning Important?\n",
    "\n",
    "1. **Performance Improvement**: Proper tuning can significantly boost model accuracy\n",
    "2. **Generalization**: Helps models perform well on unseen data\n",
    "3. **Efficiency**: Optimizes computational resources\n",
    "4. **Robustness**: Creates more stable and reliable models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084517d",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation {#dataset}\n",
    "\n",
    "Let's start by importing necessary libraries and preparing our dataset. We'll use multiple datasets to demonstrate different aspects of each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b7994f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# SHAP for model interpretability\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Bayesian optimization\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskopt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BayesSearchCV\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer, load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "\n",
    "# Model imports\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "import xgboost as xgb\n",
    "\n",
    "# SHAP for model interpretability\n",
    "import shap\n",
    "\n",
    "# Bayesian optimization\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic datasets for demonstration\n",
    "print(\"Creating datasets...\")\n",
    "\n",
    "# Classification dataset\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    n_classes=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=15,\n",
    "    noise=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Load real datasets\n",
    "breast_cancer = load_breast_cancer()\n",
    "X_bc, y_bc = breast_cancer.data, breast_cancer.target\n",
    "\n",
    "# Split datasets\n",
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_bc_train, X_bc_test, y_bc_train, y_bc_test = train_test_split(\n",
    "    X_bc, y_bc, test_size=0.2, random_state=42, stratify=y_bc\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_class = StandardScaler()\n",
    "scaler_reg = StandardScaler()\n",
    "scaler_bc = StandardScaler()\n",
    "\n",
    "X_class_train_scaled = scaler_class.fit_transform(X_class_train)\n",
    "X_class_test_scaled = scaler_class.transform(X_class_test)\n",
    "\n",
    "X_reg_train_scaled = scaler_reg.fit_transform(X_reg_train)\n",
    "X_reg_test_scaled = scaler_reg.transform(X_reg_test)\n",
    "\n",
    "X_bc_train_scaled = scaler_bc.fit_transform(X_bc_train)\n",
    "X_bc_test_scaled = scaler_bc.transform(X_bc_test)\n",
    "\n",
    "print(f\"Classification dataset: {X_class.shape[0]} samples, {X_class.shape[1]} features\")\n",
    "print(f\"Regression dataset: {X_reg.shape[0]} samples, {X_reg.shape[1]} features\")\n",
    "print(f\"Breast cancer dataset: {X_bc.shape[0]} samples, {X_bc.shape[1]} features\")\n",
    "print(\"\\nDatasets prepared and scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363741dd",
   "metadata": {},
   "source": [
    "## 3. Multi-Layer Perceptrons (MLPs) {#mlp}\n",
    "\n",
    "### Theory\n",
    "\n",
    "Multi-Layer Perceptrons are feedforward artificial neural networks that consist of:\n",
    "- **Input layer**: Receives the input features\n",
    "- **Hidden layers**: Perform transformations using activation functions\n",
    "- **Output layer**: Produces the final prediction\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- `hidden_layer_sizes`: Architecture of hidden layers\n",
    "- `activation`: Activation function ('relu', 'tanh', 'logistic')\n",
    "- `solver`: Optimization algorithm ('adam', 'lbfgs', 'sgd')\n",
    "- `alpha`: L2 regularization parameter\n",
    "- `learning_rate`: Learning rate schedule\n",
    "- `max_iter`: Maximum number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4119f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Multi-Layer Perceptrons (MLPs) ===\")\n",
    "print()\n",
    "\n",
    "# MLP Classification\n",
    "print(\"1. MLP Classification\")\n",
    "\n",
    "# Define hyperparameter grid for MLP Classification\n",
    "mlp_class_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50), (100, 50, 25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Create MLP classifier\n",
    "mlp_class = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing grid search for MLP Classification...\")\n",
    "mlp_class_grid = GridSearchCV(\n",
    "    mlp_class, \n",
    "    mlp_class_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "mlp_class_grid.fit(X_class_train_scaled, y_class_train)\n",
    "\n",
    "print(f\"Best parameters: {mlp_class_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {mlp_class_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "mlp_class_pred = mlp_class_grid.predict(X_class_test_scaled)\n",
    "mlp_class_accuracy = accuracy_score(y_class_test, mlp_class_pred)\n",
    "print(f\"Test accuracy: {mlp_class_accuracy:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Regression\n",
    "print(\"2. MLP Regression\")\n",
    "\n",
    "# Define hyperparameter grid for MLP Regression\n",
    "mlp_reg_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Create MLP regressor\n",
    "mlp_reg = MLPRegressor(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing grid search for MLP Regression...\")\n",
    "mlp_reg_grid = GridSearchCV(\n",
    "    mlp_reg, \n",
    "    mlp_reg_param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "mlp_reg_grid.fit(X_reg_train_scaled, y_reg_train)\n",
    "\n",
    "print(f\"Best parameters: {mlp_reg_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {mlp_reg_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "mlp_reg_pred = mlp_reg_grid.predict(X_reg_test_scaled)\n",
    "mlp_reg_mse = mean_squared_error(y_reg_test, mlp_reg_pred)\n",
    "mlp_reg_r2 = r2_score(y_reg_test, mlp_reg_pred)\n",
    "print(f\"Test MSE: {mlp_reg_mse:.4f}\")\n",
    "print(f\"Test R²: {mlp_reg_r2:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b89103",
   "metadata": {},
   "source": [
    "## 4. Random Forest {#random-forest}\n",
    "\n",
    "### Theory\n",
    "\n",
    "Random Forest is an ensemble method that combines multiple decision trees using:\n",
    "- **Bootstrap Aggregating (Bagging)**: Each tree is trained on a random subset of data\n",
    "- **Feature Randomness**: Each split considers only a random subset of features\n",
    "- **Voting/Averaging**: Final prediction is made by majority vote (classification) or average (regression)\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- `n_estimators`: Number of trees in the forest\n",
    "- `max_depth`: Maximum depth of trees\n",
    "- `min_samples_split`: Minimum samples required to split a node\n",
    "- `min_samples_leaf`: Minimum samples required at a leaf node\n",
    "- `max_features`: Number of features to consider for best split\n",
    "- `bootstrap`: Whether to use bootstrap sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2258eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Random Forest ===\")\n",
    "print()\n",
    "\n",
    "# Random Forest Classification\n",
    "print(\"1. Random Forest Classification\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_class_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create Random Forest classifier\n",
    "rf_class = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "print(\"Performing randomized search for Random Forest Classification...\")\n",
    "rf_class_random = RandomizedSearchCV(\n",
    "    rf_class, \n",
    "    rf_class_param_grid, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_class_random.fit(X_class_train, y_class_train)\n",
    "\n",
    "print(f\"Best parameters: {rf_class_random.best_params_}\")\n",
    "print(f\"Best cross-validation score: {rf_class_random.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "rf_class_pred = rf_class_random.predict(X_class_test)\n",
    "rf_class_accuracy = accuracy_score(y_class_test, rf_class_pred)\n",
    "print(f\"Test accuracy: {rf_class_accuracy:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbede89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance visualization\n",
    "print(\"Feature Importance Analysis\")\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = rf_class_random.best_estimator_.feature_importances_\n",
    "\n",
    "# Create feature importance plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]\n",
    "indices = np.argsort(feature_importance)[::-1][:10]  # Top 10 features\n",
    "\n",
    "plt.bar(range(10), feature_importance[indices])\n",
    "plt.xticks(range(10), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.title('Top 10 Feature Importances - Random Forest')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92020c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression\n",
    "print(\"2. Random Forest Regression\")\n",
    "\n",
    "# Define hyperparameter grid for regression\n",
    "rf_reg_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create Random Forest regressor\n",
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform randomized search\n",
    "print(\"Performing randomized search for Random Forest Regression...\")\n",
    "rf_reg_random = RandomizedSearchCV(\n",
    "    rf_reg, \n",
    "    rf_reg_param_grid, \n",
    "    n_iter=30,\n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_reg_random.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "print(f\"Best parameters: {rf_reg_random.best_params_}\")\n",
    "print(f\"Best cross-validation score: {rf_reg_random.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "rf_reg_pred = rf_reg_random.predict(X_reg_test)\n",
    "rf_reg_mse = mean_squared_error(y_reg_test, rf_reg_pred)\n",
    "rf_reg_r2 = r2_score(y_reg_test, rf_reg_pred)\n",
    "print(f\"Test MSE: {rf_reg_mse:.4f}\")\n",
    "print(f\"Test R²: {rf_reg_r2:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73fe7a3",
   "metadata": {},
   "source": [
    "## 5. Support Vector Machines (SVM) {#svm}\n",
    "\n",
    "### Theory\n",
    "\n",
    "Support Vector Machines work by:\n",
    "- Finding the optimal hyperplane that separates classes with maximum margin\n",
    "- Using kernel functions to transform data into higher-dimensional spaces\n",
    "- Focusing on support vectors (data points closest to the decision boundary)\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- `C`: Regularization parameter (controls trade-off between smooth decision boundary and classifying training points correctly)\n",
    "- `kernel`: Kernel function ('linear', 'poly', 'rbf', 'sigmoid')\n",
    "- `gamma`: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "- `degree`: Degree of polynomial kernel\n",
    "- `coef0`: Independent term in kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de997a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Support Vector Machines (SVM) ===\")\n",
    "print()\n",
    "\n",
    "# SVM Classification\n",
    "print(\"1. SVM Classification\")\n",
    "\n",
    "# Define hyperparameter grid for different kernels\n",
    "svm_class_param_grid = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'degree': [2, 3, 4],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create SVM classifier\n",
    "svm_class = SVC(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing grid search for SVM Classification...\")\n",
    "svm_class_grid = GridSearchCV(\n",
    "    svm_class, \n",
    "    svm_class_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "svm_class_grid.fit(X_bc_train_scaled, y_bc_train)\n",
    "\n",
    "print(f\"Best parameters: {svm_class_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {svm_class_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "svm_class_pred = svm_class_grid.predict(X_bc_test_scaled)\n",
    "svm_class_accuracy = accuracy_score(y_bc_test, svm_class_pred)\n",
    "print(f\"Test accuracy: {svm_class_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_bc_test, svm_class_pred, target_names=['Malignant', 'Benign']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Regression\n",
    "print(\"2. SVM Regression\")\n",
    "\n",
    "# Define hyperparameter grid for SVR\n",
    "svm_reg_param_grid = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create SVR\n",
    "svm_reg = SVR()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing grid search for SVM Regression...\")\n",
    "svm_reg_grid = GridSearchCV(\n",
    "    svm_reg, \n",
    "    svm_reg_param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "svm_reg_grid.fit(X_reg_train_scaled, y_reg_train)\n",
    "\n",
    "print(f\"Best parameters: {svm_reg_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {svm_reg_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "svm_reg_pred = svm_reg_grid.predict(X_reg_test_scaled)\n",
    "svm_reg_mse = mean_squared_error(y_reg_test, svm_reg_pred)\n",
    "svm_reg_r2 = r2_score(y_reg_test, svm_reg_pred)\n",
    "print(f\"Test MSE: {svm_reg_mse:.4f}\")\n",
    "print(f\"Test R²: {svm_reg_r2:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7b87b",
   "metadata": {},
   "source": [
    "## 6. XGBoost {#xgboost}\n",
    "\n",
    "### Theory\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an optimized gradient boosting framework that:\n",
    "- Uses gradient boosting to combine weak learners (decision trees)\n",
    "- Implements advanced regularization techniques\n",
    "- Optimizes for speed and performance\n",
    "- Handles missing values automatically\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- `n_estimators`: Number of boosting rounds\n",
    "- `learning_rate`: Step size shrinkage to prevent overfitting\n",
    "- `max_depth`: Maximum depth of trees\n",
    "- `min_child_weight`: Minimum sum of instance weight needed in a child\n",
    "- `subsample`: Subsample ratio of training instances\n",
    "- `colsample_bytree`: Subsample ratio of columns when constructing each tree\n",
    "- `reg_alpha`: L1 regularization term\n",
    "- `reg_lambda`: L2 regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051681bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== XGBoost ===\")\n",
    "print()\n",
    "\n",
    "# XGBoost Classification\n",
    "print(\"1. XGBoost Classification\")\n",
    "\n",
    "# Define hyperparameter space for Bayesian optimization\n",
    "xgb_class_param_space = {\n",
    "    'n_estimators': Integer(50, 500),\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'max_depth': Integer(3, 10),\n",
    "    'min_child_weight': Integer(1, 10),\n",
    "    'subsample': Real(0.6, 1.0),\n",
    "    'colsample_bytree': Real(0.6, 1.0),\n",
    "    'reg_alpha': Real(0.0, 1.0),\n",
    "    'reg_lambda': Real(0.0, 1.0)\n",
    "}\n",
    "\n",
    "# Create XGBoost classifier\n",
    "xgb_class = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "print(\"Performing Bayesian optimization for XGBoost Classification...\")\n",
    "xgb_class_bayes = BayesSearchCV(\n",
    "    xgb_class,\n",
    "    xgb_class_param_space,\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_class_bayes.fit(X_class_train, y_class_train)\n",
    "\n",
    "print(f\"Best parameters: {xgb_class_bayes.best_params_}\")\n",
    "print(f\"Best cross-validation score: {xgb_class_bayes.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "xgb_class_pred = xgb_class_bayes.predict(X_class_test)\n",
    "xgb_class_accuracy = accuracy_score(y_class_test, xgb_class_pred)\n",
    "print(f\"Test accuracy: {xgb_class_accuracy:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for XGBoost\n",
    "print(\"XGBoost Feature Importance Analysis\")\n",
    "\n",
    "# Get feature importances\n",
    "xgb_feature_importance = xgb_class_bayes.best_estimator_.feature_importances_\n",
    "\n",
    "# Create feature importance plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_names = [f'Feature_{i}' for i in range(len(xgb_feature_importance))]\n",
    "indices = np.argsort(xgb_feature_importance)[::-1][:10]\n",
    "\n",
    "plt.bar(range(10), xgb_feature_importance[indices])\n",
    "plt.xticks(range(10), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.title('Top 10 Feature Importances - XGBoost')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2354231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Regression\n",
    "print(\"2. XGBoost Regression\")\n",
    "\n",
    "# Define hyperparameter space for regression\n",
    "xgb_reg_param_space = {\n",
    "    'n_estimators': Integer(50, 300),\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'max_depth': Integer(3, 8),\n",
    "    'min_child_weight': Integer(1, 6),\n",
    "    'subsample': Real(0.7, 1.0),\n",
    "    'colsample_bytree': Real(0.7, 1.0)\n",
    "}\n",
    "\n",
    "# Create XGBoost regressor\n",
    "xgb_reg = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "print(\"Performing Bayesian optimization for XGBoost Regression...\")\n",
    "xgb_reg_bayes = BayesSearchCV(\n",
    "    xgb_reg,\n",
    "    xgb_reg_param_space,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_reg_bayes.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "print(f\"Best parameters: {xgb_reg_bayes.best_params_}\")\n",
    "print(f\"Best cross-validation score: {xgb_reg_bayes.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "xgb_reg_pred = xgb_reg_bayes.predict(X_reg_test)\n",
    "xgb_reg_mse = mean_squared_error(y_reg_test, xgb_reg_pred)\n",
    "xgb_reg_r2 = r2_score(y_reg_test, xgb_reg_pred)\n",
    "print(f\"Test MSE: {xgb_reg_mse:.4f}\")\n",
    "print(f\"Test R²: {xgb_reg_r2:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3c2ea",
   "metadata": {},
   "source": [
    "## 7. Model Interpretability with SHAP {#shap}\n",
    "\n",
    "### Theory\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a unified framework for explaining machine learning model predictions by:\n",
    "- Computing Shapley values from cooperative game theory\n",
    "- Providing consistent and locally accurate explanations\n",
    "- Showing how each feature contributes to individual predictions\n",
    "- Offering both global and local interpretability\n",
    "\n",
    "### Types of SHAP Explanations:\n",
    "- **Local explanations**: Why did the model make a specific prediction?\n",
    "- **Global explanations**: How does each feature affect the model overall?\n",
    "- **Feature interactions**: How do features work together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SHAP Model Interpretability ===\")\n",
    "print()\n",
    "\n",
    "# Initialize SHAP\n",
    "shap.initjs()\n",
    "\n",
    "# 1. SHAP for Random Forest (Tree Explainer)\n",
    "print(\"1. SHAP Analysis for Random Forest\")\n",
    "\n",
    "# Create Tree Explainer for Random Forest\n",
    "rf_explainer = shap.TreeExplainer(rf_class_random.best_estimator_)\n",
    "rf_shap_values = rf_explainer.shap_values(X_class_test)\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(rf_shap_values[0], X_class_test, feature_names=[f'Feature_{i}' for i in range(X_class_test.shape[1])], show=False)\n",
    "plt.title('SHAP Summary Plot - Random Forest (Class 0)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SHAP for XGBoost\n",
    "print(\"2. SHAP Analysis for XGBoost\")\n",
    "\n",
    "# Create Tree Explainer for XGBoost\n",
    "xgb_explainer = shap.TreeExplainer(xgb_class_bayes.best_estimator_)\n",
    "xgb_shap_values = xgb_explainer.shap_values(X_class_test)\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(xgb_shap_values, X_class_test, feature_names=[f'Feature_{i}' for i in range(X_class_test.shape[1])], show=False)\n",
    "plt.title('SHAP Summary Plot - XGBoost')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SHAP for SVM (using Kernel Explainer)\n",
    "print(\"3. SHAP Analysis for SVM\")\n",
    "\n",
    "# Create a smaller sample for SVM SHAP analysis (KernelExplainer is computationally expensive)\n",
    "sample_size = 50\n",
    "sample_indices = np.random.choice(X_bc_test_scaled.shape[0], sample_size, replace=False)\n",
    "X_sample = X_bc_test_scaled[sample_indices]\n",
    "\n",
    "# Create Kernel Explainer for SVM\n",
    "svm_explainer = shap.KernelExplainer(\n",
    "    svm_class_grid.best_estimator_.predict_proba, \n",
    "    X_bc_train_scaled[:100]  # Use a small background dataset\n",
    ")\n",
    "\n",
    "print(\"Computing SHAP values for SVM (this may take a moment...)\")\n",
    "svm_shap_values = svm_explainer.shap_values(X_sample[:10])  # Analyze first 10 samples\n",
    "\n",
    "# Create feature names for breast cancer dataset\n",
    "feature_names_bc = [f'Feature_{i}' for i in range(X_sample.shape[1])]\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(svm_shap_values[1], X_sample[:10], feature_names=feature_names_bc, show=False)\n",
    "plt.title('SHAP Summary Plot - SVM (Benign Class)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Local explanation example\n",
    "print(\"4. Local SHAP Explanation Example\")\n",
    "\n",
    "# Select a single instance for detailed explanation\n",
    "instance_idx = 0\n",
    "single_instance = X_class_test[instance_idx:instance_idx+1]\n",
    "\n",
    "# Get SHAP values for single instance\n",
    "single_shap_values = xgb_explainer.shap_values(single_instance)\n",
    "\n",
    "# Create waterfall plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values=single_shap_values[0], \n",
    "        base_values=xgb_explainer.expected_value, \n",
    "        data=single_instance[0],\n",
    "        feature_names=[f'Feature_{i}' for i in range(single_instance.shape[1])]\n",
    "    ),\n",
    "    show=False\n",
    ")\n",
    "plt.title(f'SHAP Waterfall Plot - XGBoost (Instance {instance_idx})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print prediction details\n",
    "prediction = xgb_class_bayes.best_estimator_.predict(single_instance)[0]\n",
    "probability = xgb_class_bayes.best_estimator_.predict_proba(single_instance)[0]\n",
    "print(f\"Predicted class: {prediction}\")\n",
    "print(f\"Class probabilities: {probability}\")\n",
    "print(f\"Actual class: {y_class_test[instance_idx]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f05984",
   "metadata": {},
   "source": [
    "## 8. Model Comparison and Selection {#comparison}\n",
    "\n",
    "Let's create a comprehensive comparison of all the models we've tuned to help guide model selection decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afc709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Model Comparison and Selection ===\")\n",
    "print()\n",
    "\n",
    "# Create comparison results\n",
    "classification_results = {\n",
    "    'Model': ['MLP', 'Random Forest', 'SVM', 'XGBoost'],\n",
    "    'Test Accuracy': [mlp_class_accuracy, rf_class_accuracy, svm_class_accuracy, xgb_class_accuracy],\n",
    "    'CV Score': [mlp_class_grid.best_score_, rf_class_random.best_score_, svm_class_grid.best_score_, xgb_class_bayes.best_score_]\n",
    "}\n",
    "\n",
    "# Note: For regression results, we'll use the available models\n",
    "regression_results = {\n",
    "    'Model': ['MLP', 'Random Forest', 'SVM', 'XGBoost'],\n",
    "    'Test MSE': [mlp_reg_mse, rf_reg_mse, svm_reg_mse, xgb_reg_mse],\n",
    "    'Test R²': [mlp_reg_r2, rf_reg_r2, svm_reg_r2, xgb_reg_r2]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "class_df = pd.DataFrame(classification_results)\n",
    "reg_df = pd.DataFrame(regression_results)\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(class_df.round(4))\n",
    "print()\n",
    "\n",
    "print(\"Regression Results:\")\n",
    "print(reg_df.round(4))\n",
    "print()\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Classification accuracy comparison\n",
    "axes[0, 0].bar(class_df['Model'], class_df['Test Accuracy'])\n",
    "axes[0, 0].set_title('Classification Test Accuracy')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Classification CV score comparison\n",
    "axes[0, 1].bar(class_df['Model'], class_df['CV Score'])\n",
    "axes[0, 1].set_title('Classification CV Score')\n",
    "axes[0, 1].set_ylabel('CV Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Regression MSE comparison\n",
    "axes[1, 0].bar(reg_df['Model'], reg_df['Test MSE'])\n",
    "axes[1, 0].set_title('Regression Test MSE')\n",
    "axes[1, 0].set_ylabel('MSE')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Regression R² comparison\n",
    "axes[1, 1].bar(reg_df['Model'], reg_df['Test R²'])\n",
    "axes[1, 1].set_title('Regression Test R²')\n",
    "axes[1, 1].set_ylabel('R²')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection guidance\n",
    "print(\"=== Model Selection Guidance ===\")\n",
    "print()\n",
    "\n",
    "guidance = {\n",
    "    'Model': ['MLP', 'Random Forest', 'SVM', 'XGBoost'],\n",
    "    'Best Use Cases': [\n",
    "        'Complex non-linear patterns, large datasets, feature learning',\n",
    "        'Interpretable models, handles mixed data types, robust to outliers',\n",
    "        'High-dimensional data, small datasets, strong theoretical foundation',\n",
    "        'Structured data, competitions, handles missing values, high performance'\n",
    "    ],\n",
    "    'Pros': [\n",
    "        'Flexible, learns complex patterns, no assumptions about data distribution',\n",
    "        'Feature importance, handles overfitting well, fast training',\n",
    "        'Memory efficient, works well with small datasets, strong generalization',\n",
    "        'High performance, built-in regularization, handles missing values'\n",
    "    ],\n",
    "    'Cons': [\n",
    "        'Black box, requires tuning, sensitive to scaling',\n",
    "        'Can overfit with noisy data, biased toward categorical features',\n",
    "        'Slow on large datasets, sensitive to feature scaling, difficult to interpret',\n",
    "        'Many hyperparameters, can overfit, requires careful tuning'\n",
    "    ]\n",
    "}\n",
    "\n",
    "guidance_df = pd.DataFrame(guidance)\n",
    "print(guidance_df.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615599ee",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Best Practices {#conclusion}\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Model Selection**: No single model works best for all problems. Consider:\n",
    "   - Dataset size and dimensionality\n",
    "   - Interpretability requirements\n",
    "   - Training time constraints\n",
    "   - Performance requirements\n",
    "\n",
    "2. **Hyperparameter Tuning Strategies**:\n",
    "   - **Grid Search**: Exhaustive but computationally expensive\n",
    "   - **Random Search**: More efficient for high-dimensional spaces\n",
    "   - **Bayesian Optimization**: Smart search using probabilistic models\n",
    "\n",
    "3. **Cross-Validation**: Always use cross-validation to:\n",
    "   - Get reliable performance estimates\n",
    "   - Avoid overfitting to validation data\n",
    "   - Compare models fairly\n",
    "\n",
    "4. **Feature Scaling**: Critical for:\n",
    "   - Neural networks (MLPs)\n",
    "   - Support Vector Machines\n",
    "   - Less important for tree-based methods\n",
    "\n",
    "5. **Model Interpretability**: Use SHAP for:\n",
    "   - Understanding model decisions\n",
    "   - Debugging model behavior\n",
    "   - Building trust with stakeholders\n",
    "   - Identifying important features\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start Simple**: Begin with simpler models before moving to complex ones\n",
    "2. **Baseline Models**: Always establish a baseline (e.g., majority class, mean prediction)\n",
    "3. **Data Quality**: Ensure high-quality data through proper preprocessing\n",
    "4. **Validation Strategy**: Use appropriate validation techniques for your problem type\n",
    "5. **Performance Metrics**: Choose metrics that align with your business objectives\n",
    "6. **Computational Resources**: Consider training time and inference speed requirements\n",
    "7. **Model Monitoring**: Monitor model performance in production and retrain when necessary\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with ensemble methods (combining multiple models)\n",
    "- Explore deep learning frameworks for more complex neural networks\n",
    "- Learn about automated machine learning (AutoML) tools\n",
    "- Study domain-specific modeling techniques\n",
    "- Practice with real-world datasets from your field of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aca4d6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive lesson, we covered:\n",
    "\n",
    "- **Model Tuning Fundamentals**: Understanding hyperparameters vs parameters\n",
    "- **Multi-Layer Perceptrons**: Neural networks for complex pattern recognition\n",
    "- **Random Forest**: Ensemble method with built-in feature importance\n",
    "- **Support Vector Machines**: Powerful method for high-dimensional data\n",
    "- **XGBoost**: State-of-the-art gradient boosting framework\n",
    "- **SHAP**: Model interpretability and explanation framework\n",
    "- **Model Comparison**: Systematic approach to model selection\n",
    "\n",
    "Each algorithm has its strengths and ideal use cases. The key to successful machine learning is understanding when to apply each method and how to tune them effectively for your specific problem.\n",
    "\n",
    "**Remember**: The best model is not always the most complex one, but the one that generalizes well to new data while meeting your specific requirements for interpretability, speed, and accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
